{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ginger 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system:\n",
    "You are an AI assistant designed to provide helpful, accurate, and concise answers to a wide range of questions. Your responses should be clear, friendly, and professional. If you encounter a question that you cannot answer, politely inform the user and suggest that they provide more specific details or ask a different question. Always prioritize providing correct information and be cautious with topics that require specialized knowledge or legal, medical, or financial advice. In such cases, remind users to consult with a qualified professional. Strive to understand the context of each query and adapt your responses to meet the user's needs\n",
    "  \n",
    "\n",
    "\n",
    "{% for item in chat_history %}\n",
    "user:\n",
    "{{item.inputs.question}}\n",
    "assistant:\n",
    "{{item.outputs.answer}}\n",
    "{% endfor %}\n",
    "\n",
    "user:\n",
    "{{question}}\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flow.meta.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version: \"1.0\"\n",
    "name: \"Pesport AI Model Orchestration\"\n",
    "description: |\n",
    "  A prompt flow that orchestrates text, image, and voice inputs for the Pesport app using GPT-4, DALL路E, and Whisper. Handles routing, ambiguity, and error management.\n",
    "  \n",
    "steps:\n",
    "  # Step 1: Input Classification\n",
    "  - step: \n",
    "      name: classify_input\n",
    "      type: classifier\n",
    "      description: |\n",
    "        This step will classify whether the input is text, image, or voice.\n",
    "      input: ${input_data}   # Raw input from user\n",
    "      output: ${classification}\n",
    "      classifier_rules:\n",
    "        - rule:\n",
    "            if: is_text(${input_data})\n",
    "            then: \"text\"\n",
    "        - rule:\n",
    "            if: is_image(${input_data})\n",
    "            then: \"image\"\n",
    "        - rule:\n",
    "            if: is_audio(${input_data})\n",
    "            then: \"audio\"\n",
    "        - rule:\n",
    "            if: contains_text_and_image(${input_data})\n",
    "            then: \"text_only\"  # Handles ambiguous cases like mixed input\n",
    "\n",
    "  # Step 2: Route Input to GPT-4 (Text)\n",
    "  - step: \n",
    "      name: process_text\n",
    "      type: openai\n",
    "      when: ${classification} == \"text\" || ${classification} == \"text_only\"\n",
    "      description: \"This step processes text input using GPT-4.\"\n",
    "      model: gpt-4o-mini   # GPT-4 Model\n",
    "      input:\n",
    "        prompt: ${input_data}\n",
    "        temperature: 0.8\n",
    "        max_tokens: 800\n",
    "        top_p: 0.95\n",
    "        presence_penalty: 0\n",
    "        frequency_penalty: 0\n",
    "      output: ${gpt4_response}\n",
    "\n",
    "  # Step 3: Route Input to DALL路E (Image)\n",
    "  - step: \n",
    "      name: process_image\n",
    "      type: openai\n",
    "      when: ${classification} == \"image\"\n",
    "      description: \"This step processes image input using DALL路E.\"\n",
    "      model: dalle\n",
    "      input:\n",
    "        prompt: ${input_data}\n",
    "      output: ${dalle_response}\n",
    "\n",
    "  # Step 4: Route Input to Whisper (Audio)\n",
    "  - step: \n",
    "      name: process_audio\n",
    "      type: openai\n",
    "      when: ${classification} == \"audio\"\n",
    "      description: \"This step processes voice input using Whisper.\"\n",
    "      model: whisper\n",
    "      input:\n",
    "        audio: ${input_data}\n",
    "      output: ${whisper_response}\n",
    "\n",
    "  # Step 5: Handling Multiple Models (Text + Image)\n",
    "  - step: \n",
    "      name: process_mixed_input\n",
    "      type: parallel\n",
    "      when: ${classification} == \"text_only\"\n",
    "      description: |\n",
    "        This step will process the text portion of mixed input (text + image), ignoring the image for now.\n",
    "      branches:\n",
    "        - branch: \n",
    "            steps:\n",
    "              - step: \n",
    "                  name: handle_text_part\n",
    "                  type: openai\n",
    "                  model: gpt-4o-mini\n",
    "                  input:\n",
    "                    prompt: extract_text(${input_data})\n",
    "                    temperature: 0.8\n",
    "                    max_tokens: 800\n",
    "                    top_p: 0.95\n",
    "                    presence_penalty: 0\n",
    "                    frequency_penalty: 0\n",
    "                  output: ${gpt4_response}\n",
    "\n",
    "  # Step 6: Error Handling\n",
    "  - step:\n",
    "      name: error_handling\n",
    "      type: error_handler\n",
    "      description: |\n",
    "        Logs and handles errors encountered during classification or model invocation.\n",
    "      actions:\n",
    "        - action: log_error(${classification_error})\n",
    "        - action: respond_with_message(\"We encountered an issue with your input. Please try again.\")\n",
    "        \n",
    "  # Step 7: Success Response\n",
    "  - step:\n",
    "      name: success_response\n",
    "      type: response\n",
    "      description: |\n",
    "        Returns the final response from the appropriate model (GPT-4, DALL路E, or Whisper).\n",
    "      when: ${classification} in [\"text\", \"image\", \"audio\", \"text_only\"]\n",
    "      input:\n",
    "        response_text: \n",
    "          - ${gpt4_response}\n",
    "          - ${dalle_response}\n",
    "          - ${whisper_response}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
